# Progress Log

## Project State (2026-02-12)

Existing frameworks: pydantic_ai, langgraph, smolagents — all fully implemented across 4 scenarios.

### Critical Conventions (read before every iteration)

- RAGFramework Protocol: `name` property, `async ingest()`, `async query()`, `async cleanup()`
- ConfigurableFramework: optional `configure(mode, scenario_name, scenario_type, scenario_config, mode_config)`
- Constructor signature: `__init__(self, model: str = MODEL, embedding_store: EmbeddingStore | None = None)`
- smolagents is the exception: uses `model_id` instead of `model` parameter
- UsageStats fields: prompt_tokens, completion_tokens, total_tokens, latency_seconds, model_name
- Registration in run_eval.py requires 3 places: _FRAMEWORK_CLASS_NAMES dict, FRAMEWORKS list, _format_model_for_framework()
- _format_model_for_framework() maps raw model names to framework-specific format:
  - pydantic_ai: "anthropic:model" or "openai:model"
  - langgraph: raw model name (routing handled by _make_llm helper)
  - smolagents: "anthropic/model" or "openai/model" (LiteLLM format)
- Root pyproject.toml workspace members must include the new framework path
- uv sync --all-packages to install after adding a new workspace member
- If uv sync fails with tiktoken errors: rm -rf .venv && uv sync --all-packages
- load_dotenv(override=True) is used in run_eval.py to handle empty env vars
- GPT-5-mini does not support temperature=0 or stop sequences
- Test with: uv run python scripts/run_eval.py --framework <name> --scenario rag_qa --skip-code-review

### Reference implementation pattern (from pydantic_ai/rag_qa.py)

1. Module-level MODEL constant (default "gpt-5-mini")
2. Class with __init__(model, embedding_store) storing both
3. ingest(): if embedding_store provided, skip embedding; otherwise use chromadb + OpenAI embeddings
4. query(): retrieve via embedding_store or chromadb, call LLM, measure latency, extract tokens
5. cleanup(): delete chromadb collection if created locally
6. Return RunResult(answer=Answer(...), usage=UsageStats(...))

## FW-001: CrewAI rag_qa implementation (2026-02-12)

### What was implemented
- Created `frameworks/crewai/` directory with pyproject.toml, AGENTS.md, src/impl_crewai/__init__.py, rag_qa.py
- `CrewAIRAG` class implementing RAGFramework Protocol
- Registered in run_eval.py (3 places) and root pyproject.toml workspace members

### Files changed
- `frameworks/crewai/pyproject.toml` — new, depends on crewai>=0.108, chromadb, openai, shared
- `frameworks/crewai/AGENTS.md` — new, documents architecture and key APIs
- `frameworks/crewai/src/impl_crewai/__init__.py` — new, package init
- `frameworks/crewai/src/impl_crewai/rag_qa.py` — new, main implementation
- `scripts/run_eval.py` — added crewai to sys.path, _FRAMEWORK_CLASS_NAMES, FRAMEWORKS, _format_model_for_framework
- `pyproject.toml` — added "frameworks/crewai" to workspace members

### Results
- Avg Correctness: 4.4/5, Completeness: 4.4/5, Faithfulness: 5.0/5
- Retrieval Precision: 1.00, Recall: 1.00
- Total Tokens: 50912, Avg Latency: 13.13s

### Learnings and gotchas
- CrewAI uses LiteLLM format for model names: `"openai/gpt-5-mini"` (same as smolagents)
- `_format_model_for_framework()` groups crewai with smolagents for `"provider/model"` format
- `crew.akickoff()` does NOT exist in crewai 1.6.1 — use `crew.kickoff_async()` instead
- `result.token_usage` is a Pydantic `UsageMetrics` model (not a dict) — access via `.prompt_tokens`, `.completion_tokens`, `.total_tokens` attributes
- Must set `CREWAI_TRACING_ENABLED=false` env var to avoid interactive tracing prompt during eval
- CrewAI Agent reuses across queries but Task+Crew must be created per query (task description varies)
- `verbose=False` on both Agent and Crew to suppress internal logging
- CrewAI is heavier than other frameworks for simple RAG — higher token usage due to agent system prompts

## FW-002: CrewAI multihop_qa implementation (2026-02-12)

### What was implemented
- `frameworks/crewai/src/impl_crewai/multihop_qa.py` — multihop QA with planning, validation, and iterative retrieval
- Same `CrewAIRAG` class name (required by framework registration)
- Baseline: single retrieval pass + generate (single Crew run)
- Capability: planner Crew → retrieve subqueries → answer Crew → checker Crew → optional re-retrieve + re-answer

### Files changed
- `frameworks/crewai/src/impl_crewai/multihop_qa.py` — new

### Results
- Baseline: correctness 3.9/5, completeness 3.7/5, faithfulness 4.7/5, tokens 85K
- Capability: correctness 4.1/5, completeness 3.8/5, faithfulness 4.8/5, tokens 1.3M, latency 70s
- Capability mode shows improved retrieval recall (0.93 vs 0.70) and hop coverage

### Learnings and gotchas
- CrewAI has very high token usage in capability mode (~1.3M tokens for 9 questions) due to verbose agent system prompts per Crew run
- Structured output: CrewAI doesn't natively return parsed JSON from tasks; must parse JSON from raw text output
- `_parse_json_from_text()` helper needed to handle markdown code fences and extra text around JSON
- Multiple sequential Crew runs accumulate tokens — extract from each `result.token_usage` and sum
- Agents can be reused across Crew instances, reducing setup overhead
- Running 3-4 sequential Crews per question is slow but works correctly

## FW-003: CrewAI agentic_sql_qa implementation (2026-02-12)

### What was implemented
- `frameworks/crewai/src/impl_crewai/agentic_sql_qa.py` — tool-calling with CrewAI Agent
- Two custom tools: `RunSQLTool` and `LookupDocTool` (both extend `crewai.tools.BaseTool`)
- Tools delegate to shared `AgenticSQLRuntime` for actual logic
- Agent with `max_iter` controls iteration budget, runtime enforces `max_tool_calls`

### Files changed
- `frameworks/crewai/src/impl_crewai/agentic_sql_qa.py` — new

### Results
- Baseline: correctness 1.5/5 (2 questions hit OpenAI content policy errors, max_tool_calls=1 is very restrictive)
- Capability: correctness 4.4/5, completeness 4.4/5, faithfulness 4.2/5, tool_coverage 0.875

### Learnings and gotchas
- CrewAI tools extend `crewai.tools.BaseTool` (Pydantic model) with `_run()` method
- Must set `model_config = {"arbitrary_types_allowed": True}` when storing non-serializable runtime refs
- `args_schema` must be a Pydantic `BaseModel` class with `Field` descriptions for each arg
- `Agent(max_iter=N)` controls the iteration limit; set to `max_steps + 2` for buffer
- Tools are instantiated per query (fresh for each run since runtime.start_run resets state)
- Some questions hit OpenAI "invalid_prompt" content policy errors — this is an OpenAI API issue, not our code

## FW-004: CrewAI multi_agent_coordination implementation (2026-02-12)

### What was implemented
- `frameworks/crewai/src/impl_crewai/multi_agent_coordination.py` — multi-agent coordination with CrewAI
- Three custom tools: `QueryInfrastructureTool`, `QuerySecurityTool`, `LookupRunbookTool` (all extend `BaseTool`)
- Baseline: single agent with all 3 tools
- Capability: `Process.hierarchical` with coordinator as `manager_agent` + 3 specialist agents

### Files changed
- `frameworks/crewai/src/impl_crewai/multi_agent_coordination.py` — new

### Results
- Baseline: correctness 2.4/5, completeness 2.1/5, faithfulness 2.4/5, avg latency 65s, tokens 475K
- Capability (hierarchical): correctness 1.0/5, avg latency 253s — most questions timeout at 300s
- Only 2/10 questions answered in capability mode (q1 simple query, q6 coordination query)
- CrewAI's hierarchical process is very slow for complex multi-tool coordination questions

### Learnings and gotchas
- First approach used nested sub-Crews inside tool `_run()` methods — caused 253s avg latency, most timeouts
- Refactored to `Process.hierarchical` with `manager_agent` — slightly better but still slow for complex questions
- `LLM()` returns provider-specific subclass (e.g. `OpenAILLM`), not base `LLM` class — use `Any` type annotation for tool fields
- `Process.hierarchical` requires the task to NOT be assigned to an agent (no `agent=` on Task)
- Specialist agents need their own `max_iter` budget for tool calling within hierarchical crew
- `allow_delegation=True` on coordinator is needed for hierarchical delegation to work
- CrewAI's hierarchical process has high overhead per delegation round — each question may need multiple delegations
- Baseline with single agent + all tools performs better for simpler questions (less delegation overhead)

## FW-005: Run all benchmarks and update RESULTS_SUMMARY.md (2026-02-12/13)

### What was done
- Ran all 5 eval commands with `--all` across all 4 frameworks (pydantic_ai, langgraph, smolagents, crewai)
- Commands: rag_qa baseline, multihop_qa capability, agentic_sql_qa capability, multi_agent_coordination baseline+capability
- Updated RESULTS_SUMMARY.md with CrewAI columns in all tables and analysis

### Results summary (all frameworks, GPT-5-mini)
- RAG QA baseline: all 4 at parity (4.4/5)
- Multihop QA capability: LangGraph 4.1 > Pydantic AI 3.8 = CrewAI 3.8 > smolagents 3.3
- Agentic SQL capability: LangGraph 5.0 = CrewAI 5.0 > Pydantic AI 4.8 > smolagents 2.5
- Multi-agent coord capability: Pydantic AI 4.9 >> LangGraph 3.6 >> smolagents 1.0 >> CrewAI 0.2
- Multi-agent coord baseline: Pydantic AI 3.7 ≈ LangGraph 3.6 > CrewAI 2.6 > smolagents 1.4

### Key CrewAI findings
- CrewAI's best scenario is agentic SQL (5.0/5) — single-agent tool calling via BaseTool works excellently
- CrewAI's worst scenario is multi-agent coordination capability (0.2/5) — hierarchical process + stop sequence issues = almost total failure
- Token usage is consistently highest among all frameworks (7-26x more than Pydantic AI) due to CrewAI's verbose internal system prompts
- Cost is prohibitive for complex scenarios ($1.91 for multihop, $3.06 for multi-agent coordination)

### Files changed
- `RESULTS_SUMMARY.md` — updated all tables with CrewAI column, added all-scenario summary, updated analysis
- `prd.json` — set FW-005 passes to true
- `results/comparison_*.md` — all 5 comparison reports regenerated with 4-framework data
